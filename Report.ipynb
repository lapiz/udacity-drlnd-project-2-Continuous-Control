{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-master",
   "metadata": {},
   "source": [
    "# Report\n",
    "---\n",
    "In this notebook, We see my implementation of the second project of the Deep Reinforcement Learning Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-simon",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Summary\n",
    "I implement a DDPG. Based on DRLND Project Sample.\n",
    "\n",
    "### Details\n",
    "Actor and Critic have following Neural Networks.\n",
    "\n",
    "#### Actor\n",
    "- First fully connected layer\n",
    "  - inputs (states) -> $N_{a1}$ \n",
    "- Second fully connected layer\n",
    "  - $N_{a1}$ -> $N_{a2}$\n",
    "- Third fully connected layer\n",
    "  - $N_{a2}$ -> outputs (actions)\n",
    "\n",
    "#### Critic\n",
    "- First fully connected layer\n",
    "  - inputs (states) -> $N_{c1}$ \n",
    "- Second fully connected layer\n",
    "  - $N_{c1}$ + (actions) -> $N_{c2}$\n",
    "- Third fully connected layer\n",
    "  - $N_{c2}$ -> output (Q)\n",
    " \n",
    "### Hyperparameters\n",
    "\n",
    "Agent hyperparameters are:\n",
    "\n",
    "|parameter | value | description |\n",
    "|----------|-------|-------------|\n",
    "|buffer_size|1000000| Number of experiences to hold in the replay memory |\n",
    "|batch_size|128| Minibatch size used at each step |\n",
    "|gamma | 0.95 | Discount applied to future rewards |\n",
    "|tau | 0.001 | Scaling parameter applied to target update |\n",
    "|LR/Actor| 0.0001 | Actor Learning rate for Adam optimizer |\n",
    "|LR/Critic| 0.0001 | Actor Learning rate for Adam optimizer |\n",
    "|learn_per_update | 20 | Number of agent steps between update oprations |\n",
    "|update_times| 10 | Numer of samples for single update |\n",
    "|hidden_layer/Actor| (256, 128) | Number of nodes for actor hidden layer ($N_{a1}$, $N_{a2}$)|\n",
    "|hidden_layer/Critic| (256, 128) | Number of nodes for critic hidden layer ($N_{c1}$, $N_{c2}$)|\n",
    "|weight_decay| 0 | L2 penalty for Critic Adam optimizer |\n",
    "\n",
    "\n",
    "Training parameters are:\n",
    "\n",
    "|parameter | value | description |\n",
    "|----------|-------|-------------|\n",
    "|epoch  | 1000 | Maximum number of training episodes |\n",
    "|t_max  | 1000 | Maximum step of single episode |\n",
    "|seed   |  0 | random seed |\n",
    "|scores/expectation | 30 | Expected score to solved | \n",
    "|scores/window_size | 30 | moving average window size | \n",
    "|scores/check_solved | True | Finish when moving average reach expectation | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-reflection",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Training \n",
    "\n",
    "[Trainng notebook](Continuous_Control.ipynb)\n",
    "\n",
    "### Run with trained data\n",
    "\n",
    "[Runwiht trained data notebook](run_trained.ipynb)\n",
    "\n",
    "### Plot of Rewards\n",
    "\n",
    "![Plot of Reward](reacher_result_scores.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-anchor",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "- Implement D4PG (Distributed Distributional Deterministic Policy Gradients) \n",
    "- Change NN Model\n",
    "  - More hidden layers\n",
    "  - More nodes in hidden layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
